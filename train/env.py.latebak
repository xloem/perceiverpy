import gym, gym.spaces

import time
import torch
import pytorch_model
import data_items




class Test:
    def __init__(self):
        import torchvision
        def permute(tensor):
            return tensor.permute(1,2,0)
        transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), permute])
        self.trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
        self.testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
        #testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=True, num_workers=2)
        self.classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    
        import perceiver_pytorch as pp
        model = pytorch_model.NamedModel(pp.Perceiver, input_channels=3, input_axis=2, fourier_encode_data=True, num_freq_bands=6, max_freq=10.0, depth=6, num_latents=32, latent_dim=128, cross_heads=1, latent_heads=2, cross_dim_head=8, latent_dim_head=8, num_classes=10, attn_dropout=0.0, ff_dropout=0.0, weight_tie_layers=False)
        #print(next(model.parameters()), 'sometimes i see model parameters that start for some reason all near zero.  that is a bug.  parameters above should be near unit magnitude.')
        criterion = torch.nn.CrossEntropyLoss(reduction='none')
        optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)
    
        self.model = pytorch_model.PytorchModel(model, criterion, optimizer, name_suffix='-'+str(time.time()))

#testsetup = Test()
#trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
#dataidx = 0
class Trainer(gym.Env):
    actions = 'LEAVE PREDICT TRAIN'.split(' ')
    for index, action in enumerate(actions):
        locals()[action] = index
    def __init__(self, model = None, datalist = None, initial_loss=10, maxcachesize=64, maxbatchsize=256, maxdata=64):
        if model is None or datalist is None:
            testsetup = Test()
            if model is None:
                model = testsetup.model
            if datalist is None:
                datalist = testsetup.trainset
        self.model = model
        self.datalist = datalist
        self.initial_loss = initial_loss
        self.maxcachesize = min(maxdata, maxcachesize)
        self.maxbatchsize = maxbatchsize
        self.maxdata = len(self.datalist) if maxdata is None else maxdata

        action_space_bounds = []
        action_space_bounds.extend([len(Trainer.actions)] * (self.maxcachesize)) # items to predict or train
        #action_space_bounds.append(1024*1024) # number of history items to walk back by

        self.action_space = gym.spaces.MultiDiscrete(action_space_bounds)
        self.observation_space = gym.spaces.Box(low=0,high=initial_loss,shape=(2,self.maxcachesize))
        self.reward_range = (-float('inf'),0)


        self.initial_loss = initial_loss
        self.initial_snapshot = model.to_snapshot()
    def reset(self):
        #self.datait = iter(self.datalist)
        self.dataidx = 0
        self.last_time = time.time()
        self.dataperm = torch.randperm(self.maxcachesize)
        self.cache = [Trainer.CachedItem(idx) for idx in range(self.maxcachesize)]
        self.model.from_snapshot(self.initial_snapshot)
        self.cache[0].item = self._getitem()
        self.cache[0].item.loss(self.model)
        self.observation = torch.tensor([[0.0]*len(self.cache)]*2)
        return self._observe()
    class CachedItem:
        def __init__(self, idx, dataitem = None):
            self.idx = idx
            self.item = dataitem
            self.trained_ct = 0
    def _observe(self):
        observation = []
        for cache in self.cache:
            if cache.item is None:
                observation.append(0)
            else:
                observation.append(cache.item.cached_loss)
        # bug with below line: passes items that are None
        #return data_items.DataItem.losses(*(cache.item for cache in self.cache), model=self.model)
        self.observation[0] = self.observation[1]
        self.observation[1] = torch.tensor(observation)
        return self.observation
    def _getitem(self):
        item = data_items.DataItem(self.datalist, self.dataperm[self.dataidx])
        self.dataidx += 1
        return item
    def step(self, a):
        trainlist = []
        predictlist = []
        self.action = a
        self.testlist = []
        replace_cand = None
        # collect item groups
        for action, cache in zip(a, self.cache):
            if cache.item is not None:
                if action == Trainer.TRAIN:
                    trainlist.append(cache.item)
                    cache.trained_ct += 1
                elif cache.trained_ct == 0:
                    self.testlist.append(cache.item)
                elif action == Trainer.PREDICT:
                    predictlist.append(cache.item)
            if replace_cand is None or (cache.item is None and replace_cand.item is not None) or cache.trained_ct < replace_cand.trained_ct:
                replace_cand = cache

        # train from selected items
        if len(trainlist) > 0 and len(trainlist) <= self.maxbatchsize:
            data_items.DataItem.train_with(*trainlist, model=self.model)

        # add a new item
        replace_cand.item = self._getitem()
        self.testlist.append(replace_cand.item)

        # calc reward as negative mean of losses multiplied by inter-step time
        self.loss = torch.mean(torch.tensor(data_items.DataItem.losses(*self.testlist, *predictlist, model=self.model)[:len(self.testlist)]))
        cur_time = time.time()
        reward = (cur_time - self.last_time) * -self.loss.item() if len(trainlist) <= self.maxbatchsize else -float('inf')
        self.last_time = cur_time

        # debugging output
        self.render()

        if self.dataidx == self.maxdata and self.loss < 0.1:
            self.model.save()
        
        observation = self._observe()
        return (observation, reward, self.dataidx >= self.maxdata, {})# or self.loss >= 3 or (self.loss <= 0.1 and torch.mean(torch.tensor([x for x in observation if x > 0])) <= 0.1), {})
    def seed(self, seed=None):
        if seed is None:
            torch.default_generator.seed()
        else:
            torch.default_generator.manual_seed(seed)
        return [torch.default_generator.initial_seed()]
        #return None
    def close(self):
        del self.datalist
        del self.model
        del self.cache
        del self.initial_snapshot
    def render(self, mode='human'):
        print(
            time.asctime(time.localtime(self.last_time)),
            self.dataidx,
            self.loss.item(),
            torch.mean(torch.tensor([x for x in self._observe()[-1] if x > 0])).item(),
            sum(self.action),
            (self.action == Trainer.PREDICT).sum(),
            (self.action == Trainer.TRAIN).sum()
        )
        print(
            [torch.argmax(item.prediction(self.model)).item() for item in self.testlist],
            '=?',
            [item.request for item in self.testlist]
        )

from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env

env = make_vec_env(Trainer, n_envs=4)

model = PPO("MlpPolicy", env, verbose=2, n_steps=256*4)
model.load('ppo-trainer')
model.learn(total_timesteps=128)
model.save('ppo-trainer')

#obs = env.reset()
#for i in range(62):
#    action, _states = model.predict(obs)
#    obs, rewards, dones, info = env.step(action)
#    #env.render()
